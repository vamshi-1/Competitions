{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5 - Walmart Unit Sales Forecast \n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "iDaBon5cSinF",
    "outputId": "ce034f77-afbc-44ec-e876-35681940e75b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "from  datetime import datetime, timedelta\n",
    "import gc\n",
    "import numpy as np, pandas as pd\n",
    "import lightgbm as lgb\n",
    "import time \n",
    "import pickle\n",
    "\n",
    "start = time.time()\n",
    "import os\n",
    "\n",
    "# # for first time you need to change from colab drive to gdrive by mounting\n",
    "# from google.colab import drive \n",
    "# drive.mount('/content/gdrive')\n",
    "# !cd /content/gdrive/My\\ Drive/\n",
    "# #os.chdir('\\\\content\\\\drive\\\\My Drive')\n",
    "# #https://stackoverflow.com/questions/50479576/google-colab-changing-directory\n",
    "# os.chdir('/content/gdrive/My Drive/Competitions/m5-forecasting-accuracy/output')\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H6QWKYBDSin8"
   },
   "source": [
    "## Notebook Credits\n",
    "\n",
    "* [First R notebook](https://www.kaggle.com/kailex/m5-forecaster-v2)\n",
    "* [Python translation](https://www.kaggle.com/kneroma/m5-forecast-v2-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables settings - dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yt1n-yihSion"
   },
   "outputs": [],
   "source": [
    "CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n",
    "         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n",
    "        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\n",
    "PRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJP9wNvkSio5"
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jtiA9bj_SipL",
    "outputId": "0e01977c-653b-44c8-882f-1daa0524c3a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2016, 5, 23, 0, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 28 \n",
    "max_lags = 57\n",
    "#tr_last = 1913 #1913    \n",
    "#fday = datetime(2016,4, 25) \n",
    "tr_last = 1941\n",
    "fday = datetime(2016,5, 23)\n",
    "fday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "id": "oKjWm4aDSipY"
   },
   "source": [
    "## Function to create dataset for model training or evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m9yTeNz7Sipn"
   },
   "outputs": [],
   "source": [
    "def create_dt(is_train = True,casefile ='evaluation', nrows = None, first_day = 1200):\n",
    "    prices = pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\", dtype = PRICE_DTYPES)\n",
    "    for col, col_dtype in PRICE_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            prices[col] = prices[col].cat.codes.astype(\"int16\")\n",
    "            prices[col] -= prices[col].min()\n",
    "            \n",
    "    cal = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\", dtype = CAL_DTYPES)\n",
    "    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n",
    "    for col, col_dtype in CAL_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            cal[col] = cal[col].cat.codes.astype(\"int16\")\n",
    "            cal[col] -= cal[col].min()\n",
    "    \n",
    "    start_day = max(1 if is_train  else tr_last-max_lags, first_day)\n",
    "    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]\n",
    "    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n",
    "    dtype = {numcol:\"float32\" for numcol in numcols} \n",
    "    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n",
    "    dt = pd.read_csv(f\"../input/m5-forecasting-accuracy/sales_train_{casefile}.csv\", \n",
    "                     nrows = nrows, usecols = catcols + numcols, dtype = dtype) \n",
    "    \n",
    "    for col in catcols:\n",
    "        if col != \"id\":\n",
    "            dt[col] = dt[col].cat.codes.astype(\"int16\")\n",
    "            dt[col] -= dt[col].min()\n",
    "    \n",
    "    if not is_train:\n",
    "        for day in range(tr_last+1, tr_last+ 28 +1):\n",
    "            dt[f\"d_{day}\"] = np.nan\n",
    "    \n",
    "    dt = pd.melt(dt,\n",
    "                  id_vars = catcols,\n",
    "                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n",
    "                  var_name = \"d\",\n",
    "                  value_name = \"sales\")\n",
    "    \n",
    "    dt = dt.merge(cal, on= \"d\", copy = False)\n",
    "    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n",
    "    \n",
    "    return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to add features of dataset with history information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cgo4hPdtSip5"
   },
   "outputs": [],
   "source": [
    "def create_fea(dt):\n",
    "    lags = [7, 28]\n",
    "    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n",
    "    for lag, lag_col in zip(lags, lag_cols):\n",
    "        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n",
    "\n",
    "    wins = [7, 28]\n",
    "    for win in wins :\n",
    "        for lag,lag_col in zip(lags, lag_cols):\n",
    "            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n",
    "\n",
    "    \n",
    "    \n",
    "    date_features = {\n",
    "        \n",
    "        \"wday\": \"weekday\",\n",
    "        \"week\": \"weekofyear\",\n",
    "        \"month\": \"month\",\n",
    "        \"quarter\": \"quarter\",\n",
    "        \"year\": \"year\",\n",
    "        \"mday\": \"day\",\n",
    "#         \"ime\": \"is_month_end\",\n",
    "#         \"ims\": \"is_month_start\",\n",
    "    }\n",
    "    \n",
    "#     dt.drop([\"d\", \"wm_yr_wk\", \"weekday\"], axis=1, inplace = True)\n",
    "    \n",
    "    for date_feat_name, date_feat_func in date_features.items():\n",
    "        if date_feat_name in dt.columns:\n",
    "            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n",
    "        else:\n",
    "            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8Char2OSiqH"
   },
   "source": [
    "## Create training data df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "FIRST_DAY = 338 # If you want to load all the data set it to '1' -->  Great  memory overflow  risk \n",
    "#neglecting 2012 year data\n",
    "df = create_dt(is_train=True, first_day= FIRST_DAY)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0Y17r0VSiqj"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TcGhcPtWSiqu"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OKdfwqhfSiq_"
   },
   "source": [
    "## Add features with history information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lk-WA8DDSirK"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "create_fea(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "stz0S2F1SirV"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xDUuzklrSire"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VAvO9avqSirn"
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select input features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-lK26KRSirv"
   },
   "outputs": [],
   "source": [
    "cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n",
    "useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]\n",
    "train_cols = df.columns[~df.columns.isin(useless_cols)]\n",
    "X_train = df[train_cols]\n",
    "y_train = df[\"sales\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-9M8ZJNQSir_"
   },
   "source": [
    "## Split training and validation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-47OvqFSisM"
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "np.random.seed(777)\n",
    "\n",
    "fake_valid_inds = np.random.choice(X_train.index.values, 2_000_000, replace = False)\n",
    "train_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\n",
    "train_data = lgb.Dataset(X_train.loc[train_inds] , label = y_train.loc[train_inds], \n",
    "                         categorical_feature=cat_feats, free_raw_data=False)\n",
    "fake_valid_data = lgb.Dataset(X_train.loc[fake_valid_inds], label = y_train.loc[fake_valid_inds],\n",
    "                              categorical_feature=cat_feats,\n",
    "                 free_raw_data=False)# This is a random sample, we're not gonna apply any time series train-test-split tricks here!\n",
    "\n",
    "load_dataset= False\n",
    "data = {\"train_data\":train_data,\"valid_data\":fake_valid_data}\n",
    "if load_dataset:\n",
    "  with open(r\"lgb_datasets.pickle\", \"rb\") as pfile:\n",
    "    data = pickle.load(pfile)    \n",
    "else:\n",
    "  with open(r\"lgb_datasets.pickle\", \"wb\") as pfile:\n",
    "    pickle.dump(data, pfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wt60c9XOSisU"
   },
   "outputs": [],
   "source": [
    "del df, X_train, y_train, fake_valid_inds,train_inds ; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUpVfJVdSise"
   },
   "source": [
    "## Specify LGBM parameters and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCZm4Y2_Sisv"
   },
   "outputs": [],
   "source": [
    " params = {\n",
    "        \"objective\" : \"poisson\",\n",
    "        \"metric\" :\"rmse\",\n",
    "        \"force_row_wise\" : True,\n",
    "        \"learning_rate\" : 0.075,\n",
    "#         \"sub_feature\" : 0.8,\n",
    "        \"sub_row\" : 0.75,\n",
    "        \"bagging_freq\" : 1,\n",
    "        \"lambda_l2\" : 0.1,\n",
    "         \"nthread\" : 2,\n",
    "        \"metric\": [\"rmse\"],\n",
    "    'verbosity': 1,\n",
    "    'num_iterations' : 2500,\n",
    "    'num_leaves': 512 ,#256,#128\n",
    "    #'max_depth': 9,\n",
    "    \"min_data_in_leaf\": 100,\n",
    "}\n",
    "%%time\n",
    "\n",
    "m_lgb = lgb.train(params, train_data, valid_sets = [fake_valid_data], verbose_eval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63MBvUQGSis8"
   },
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZweHdc3SitD"
   },
   "outputs": [],
   "source": [
    "m_lgb.save_model(\"./model.lgb\")\n",
    "#m_lgb = lgb.Booster(model_file=\"./model.lgb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to perform forecating with the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W-aUL-LjSitM"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#predict function\n",
    "def model_predict(tr_last, fday,alphas,dataset_name):\n",
    "  \"generate forecast of a 28 days\"\n",
    "  weights = [1/len(alphas)]*len(alphas)\n",
    "  sub = 0.\n",
    "\n",
    "  for icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n",
    "\n",
    "      te = create_dt(is_train=False,casefile=dataset_name)\n",
    "      cols = [f\"F{i}\" for i in range(1,29)]\n",
    "\n",
    "      for tdelta in range(0, 28):\n",
    "          day = fday + timedelta(days=tdelta)\n",
    "          print(dataset_name,tdelta, day)\n",
    "          tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()\n",
    "          create_fea(tst)\n",
    "          tst = tst.loc[tst.date == day , train_cols]\n",
    "          te.loc[te.date == day, \"sales\"] = alpha*m_lgb.predict(tst) # magic multiplier by kyakovlev\n",
    "\n",
    "      te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n",
    "  #     te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h), \n",
    "  #                                                                           \"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "      te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
    "      te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n",
    "      te_sub.fillna(0., inplace = True)\n",
    "      te_sub.sort_values(\"id\", inplace = True)\n",
    "      te_sub.reset_index(drop=True, inplace = True)\n",
    "      #te_sub.to_csv(f\"submission_{dataset_name}{icount}.csv\",index=False)\n",
    "      if icount == 0 :\n",
    "          sub = te_sub\n",
    "          sub[cols] *= weight\n",
    "      else:\n",
    "          sub[cols] += te_sub[cols]*weight\n",
    "      print(dataset_name,\"case:\",icount, alpha, weight)\n",
    "\n",
    "  if dataset_name == \"evaluation\": \n",
    "      sub[\"id\"] = sub[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "  elif dataset_name == \"validation\":\n",
    "    sub[\"id\"] = sub[\"id\"].str.replace(\"evaluation$\", \"validation\")\n",
    "  return sub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gVde_l-yOJuh"
   },
   "outputs": [],
   "source": [
    "alphas = [1.028, 1.023, 1.018]\n",
    "\n",
    "tr_last = 1913\n",
    "fday = datetime(2016,4, 25)\n",
    "sub1 = model_predict(tr_last=1913, fday=datetime(2016,4, 25),alphas=alphas,dataset_name=\"validation\")\n",
    "\n",
    "tr_last = 1941\n",
    "fday = datetime(2016,5, 23)\n",
    "sub2 = model_predict(tr_last=1941, fday=datetime(2016,5, 23),alphas=alphas,dataset_name=\"evaluation\")\n",
    "#sub2 = sub.copy()\n",
    "#sub2[\"id\"] = sub2[\"id\"].str.replace(\"evaluation$\", \"validation\")\n",
    "\n",
    "sub = pd.concat([sub1, sub2], axis=0, sort=False)\n",
    "sub.to_csv('./submission_2500iteration.csv',index=False)\n",
    "#from IPython.display import FileLink\n",
    "#FileLink('./submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQV6FSHESitT"
   },
   "outputs": [],
   "source": [
    "sub.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_C3Rp7RSitb"
   },
   "outputs": [],
   "source": [
    "sub.id.nunique(), sub[\"id\"].str.contains(\"validation$\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FS89sjPlSith"
   },
   "outputs": [],
   "source": [
    "sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Simulation Run Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LLfJ6X7vSitr"
   },
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "# calculate the simulation run time\n",
    "hours, rem = divmod(end - start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print('Run Time:')\n",
    "print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours), int(minutes), seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_giYzva5Sitx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "M5_data_process_train_v6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
